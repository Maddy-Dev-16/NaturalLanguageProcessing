{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIA0Abm05HBi"
      },
      "source": [
        "This is the initial set-up for the whole comparative study between FFNN, Vanilla RNN (No hidden state) and LSTM. We will handle the Corpus cleaning in this notbook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do16Dl1-5BBP",
        "outputId": "ed0a0b51-9657-4109-c1d5-fa256319c6b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import pickle\n",
        "import unicodedata\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt', force=True)\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBHnR7VS5lZK"
      },
      "source": [
        "Above we have downloaded the dependencies as required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QrTjdpCJ5qkG"
      },
      "outputs": [],
      "source": [
        "#########################################\n",
        "# GLOBAL CONFIGURATION\n",
        "#########################################\n",
        "\n",
        "INPUT_FILE   = \"pride_prejudice.txt\"         # Original corpus\n",
        "TRAIN_FILE   = \"trainSetPride.txt\"  # Output: Train dataset\n",
        "TEST_FILE    = \"testSetPride.txt\"   # Output: Test dataset\n",
        "VOCAB_FILE   = \"vocabPride.pkl\"     # Output: Pickled vocabulary\n",
        "\n",
        "TEST_RATIO   = 0.06            # 2.9% of sentences go to test set\n",
        "MIN_FREQ     = 2               # Min frequency to keep a word in vocab\n",
        "PAD_TOKEN    = \"<PAD>\"\n",
        "UNK_TOKEN    = \"<UNK>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "344mg2v956mp"
      },
      "source": [
        "The global configurations have been declared, now we are ready to begin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "APFlxxAQ6Ayh"
      },
      "outputs": [],
      "source": [
        "#########################################\n",
        "# CONTRACTIONS & PUNCTUATION\n",
        "#########################################\n",
        "\n",
        "CONTRACTION_MAP = {\n",
        "    \"can't\": \"cannot\", \"won't\": \"will not\", \"it's\": \"it is\",\n",
        "    \"i'm\": \"i am\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
        "    \"we're\": \"we are\", \"they're\": \"they are\", \"you're\": \"you are\",\n",
        "    \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
        "    \"haven't\": \"have not\", \"hasn't\": \"has not\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "    \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"needn't\": \"need not\"\n",
        "    }\n",
        "\n",
        "# We remove '<' and '>' from punctuation because they are used in <UNK> and <PAD>.\n",
        "PUNCTUATION = string.punctuation.replace('<', '').replace('>', '')\n",
        "\n",
        "#########################################\n",
        "# 1. TEXT PROCESSING FUNCTIONS\n",
        "#########################################\n",
        "\n",
        "def expandContractions(text):\n",
        "    \"\"\"\n",
        "    Replaces common English contractions (e.g., \"can't\" -> \"cannot\").\n",
        "    \"\"\"\n",
        "    for contr, fullForm in CONTRACTION_MAP.items():\n",
        "        pattern = re.compile(r'\\b' + re.escape(contr) + r'\\b', flags=re.IGNORECASE)\n",
        "        text = pattern.sub(fullForm, text)\n",
        "    return text\n",
        "\n",
        "def cleanLine(line):\n",
        "    \"\"\"\n",
        "    1) Normalizes Unicode.\n",
        "    2) Replaces curly quotes/apostrophes with straight ones.\n",
        "    3) Expands contractions.\n",
        "    4) Removes punctuation (including all dash variants).\n",
        "    5) Converts to lowercase.\n",
        "    \"\"\"\n",
        "    # 1) Normalize Unicode\n",
        "    line = unicodedata.normalize('NFKC', line)\n",
        "\n",
        "    # 2) Convert curly quotes to straight quotes so regex sees them\n",
        "    #    This handles cases like “ ” or ‘ ’\n",
        "    line = line.replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
        "    line = line.replace(\"“\", '\"').replace(\"”\", '\"')\n",
        "\n",
        "    # Convert em dashes/en dashes to ASCII hyphen\n",
        "    line = re.sub(r\"[—–]\", \"-\", line)\n",
        "\n",
        "    # 3) Expand contractions\n",
        "    line = expandContractions(line)\n",
        "\n",
        "    # 4) Remove punctuation (this includes ASCII hyphen '-')\n",
        "    line = re.sub(f\"[{re.escape(PUNCTUATION)}]\", \"\", line)\n",
        "\n",
        "    # 5) Lowercase\n",
        "    return line.lower().strip()\n",
        "\n",
        "def splitIntoSentences(text):\n",
        "    \"\"\"\n",
        "    Splits a large text into individual sentences using NLTK, then cleans each sentence.\n",
        "    Returns a list of cleaned sentences (strings).\n",
        "    \"\"\"\n",
        "    rawSents = sent_tokenize(text)\n",
        "    cleanedSents = []\n",
        "    for sent in rawSents:\n",
        "        c = cleanLine(sent)\n",
        "        if c:\n",
        "            cleanedSents.append(c)\n",
        "    return cleanedSents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLDapfb16bMc"
      },
      "source": [
        "Above we have processed the text, expanded contractions, removed punctuations and split the corpus into sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Xju-j5Ra6nJe"
      },
      "outputs": [],
      "source": [
        "#########################################\n",
        "# 2. VOCABULARY FUNCTIONS\n",
        "#########################################\n",
        "\n",
        "def buildVocab(sentences, minFreq):\n",
        "    \"\"\"\n",
        "    Builds word2idx and idx2word mappings.\n",
        "    - Includes <UNK> and <PAD> by default.\n",
        "    - Only words appearing >= minFreq times are added.\n",
        "    \"\"\"\n",
        "    wordCounts = Counter()\n",
        "    for sent in sentences:\n",
        "        # Each 'sent' is expected to be a string, so split into tokens\n",
        "        tokens = sent.split()\n",
        "        wordCounts.update(tokens)\n",
        "\n",
        "    # Start vocab with UNK and PAD\n",
        "    word2idx = {UNK_TOKEN: 0, PAD_TOKEN: 1}\n",
        "    idx = 2\n",
        "\n",
        "    # Sort words by frequency (descending)\n",
        "    for word, count in sorted(wordCounts.items(), key=lambda x: x[1], reverse=True):\n",
        "        if count >= minFreq:\n",
        "            word2idx[word] = idx\n",
        "            idx += 1\n",
        "\n",
        "    # Reverse mapping\n",
        "    idx2word = {i: w for w, i in word2idx.items()}\n",
        "    return word2idx, idx2word\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFyK88G-7eqV"
      },
      "source": [
        "We have now built the vocabulary , it will be later saved to vocab.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-X26MQsx7nPF"
      },
      "outputs": [],
      "source": [
        "#########################################\n",
        "# 3. CORPUS PROCESSING & SPLITTING\n",
        "#########################################\n",
        "\n",
        "def readAndProcessCorpus(inputFile):\n",
        "    \"\"\"\n",
        "    Reads the entire file and splits into cleaned sentences.\n",
        "    Returns a list of processed sentence strings.\n",
        "    \"\"\"\n",
        "    with open(inputFile, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    # Convert full text into cleaned sentences\n",
        "    processedSentences = splitIntoSentences(text)\n",
        "    return processedSentences\n",
        "\n",
        "def randomlySplitData(sentences, testRatio):\n",
        "    \"\"\"\n",
        "    Splits the list of sentences into train (85%) and test (15%) by random selection.\n",
        "    Returns trainSentences, testSentences.\n",
        "    \"\"\"\n",
        "    total = len(sentences)\n",
        "    testSize = int(total * testRatio)\n",
        "\n",
        "    # Randomly choose testSize indices for test set\n",
        "    testIndices = set(random.sample(range(total), testSize))\n",
        "\n",
        "    testSents = []\n",
        "    trainSents = []\n",
        "    for i, sent in enumerate(sentences):\n",
        "        if i in testIndices:\n",
        "            testSents.append(sent)\n",
        "        else:\n",
        "            trainSents.append(sent)\n",
        "    return trainSents, testSents\n",
        "\n",
        "def saveLines(lines, outFile):\n",
        "    \"\"\"\n",
        "    Saves a list of sentence strings to an output file, one sentence per line.\n",
        "    \"\"\"\n",
        "    with open(outFile, 'w', encoding='utf-8') as f:\n",
        "        for line in lines:\n",
        "            f.write(line + \"\\n\")\n",
        "\n",
        "def saveVocab(word2idx, idx2word, outFile):\n",
        "    \"\"\"\n",
        "    Pickles the vocabulary dictionaries for future use.\n",
        "    \"\"\"\n",
        "    with open(outFile, 'wb') as f:\n",
        "        pickle.dump((word2idx, idx2word), f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZeiXuek7xsv"
      },
      "source": [
        "Corpus splitting is done above, and functions are defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6Nki76C72Ud",
        "outputId": "1f778975-2c59-4b91-c9db-e416255293b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total sentences after cleaning: 5856\n",
            "Train sentences: 5505, Test sentences: 351\n",
            "Vocabulary size (including <UNK> & <PAD>): 3857\n",
            " Preprocessing complete.\n",
            "   - Train set saved to 'trainSetPride.txt'\n",
            "   - Test set saved to 'testSetPride.txt'\n",
            "   - Vocab saved to 'vocabPride.pkl'\n"
          ]
        }
      ],
      "source": [
        "#########################################\n",
        "# 4. MAIN PREPROCESS FUNCTION\n",
        "#########################################\n",
        "\n",
        "def preprocessCorpus(inputFile, trainFile, testFile, vocabFile, testRatio, minFreq):\n",
        "    \"\"\"\n",
        "    1) Reads & processes the inputFile into cleaned sentences.\n",
        "    2) Randomly splits them into train & test sets (85%-15%).\n",
        "    3) Builds vocabulary from train set only.\n",
        "    4) Saves train & test sets, along with the vocabulary files.\n",
        "    \"\"\"\n",
        "    # Step 1: Read & process\n",
        "    sentences = readAndProcessCorpus(inputFile)\n",
        "    print(f\"Total sentences after cleaning: {len(sentences)}\")\n",
        "\n",
        "    # Step 2: Split\n",
        "    trainSents, testSents = randomlySplitData(sentences, testRatio)\n",
        "    print(f\"Train sentences: {len(trainSents)}, Test sentences: {len(testSents)}\")\n",
        "\n",
        "    # Step 3: Build vocabulary using only train set\n",
        "    word2idx, idx2word = buildVocab(trainSents, minFreq)\n",
        "    print(f\"Vocabulary size (including <UNK> & <PAD>): {len(word2idx)}\")\n",
        "\n",
        "    # Step 4: Save data\n",
        "    saveLines(trainSents, trainFile)\n",
        "    saveLines(testSents, testFile)\n",
        "    saveVocab(word2idx, idx2word, vocabFile)\n",
        "\n",
        "    print(f\" Preprocessing complete.\\n\"\n",
        "          f\"   - Train set saved to '{trainFile}'\\n\"\n",
        "          f\"   - Test set saved to '{testFile}'\\n\"\n",
        "          f\"   - Vocab saved to '{vocabFile}'\")\n",
        "\n",
        "#########################################\n",
        "# 5. MAIN EXECUTION\n",
        "#########################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can modify these paths if needed\n",
        "    inputFile   = INPUT_FILE\n",
        "    trainFile   = TRAIN_FILE\n",
        "    testFile    = TEST_FILE\n",
        "    vocabFile   = VOCAB_FILE\n",
        "    testRatio   = TEST_RATIO\n",
        "    minFreq     = MIN_FREQ\n",
        "\n",
        "    # Perform the preprocessing\n",
        "    preprocessCorpus(inputFile, trainFile, testFile, vocabFile, testRatio, minFreq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCjdT7nT76sI"
      },
      "source": [
        "Main Function defined and called."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp2bgMn8FY1F"
      },
      "source": [
        "This is FFNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLDMXLTWpfvv",
        "outputId": "a605d4cc-96d7-4c6f-9465-7f6b8c217d5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training FFNN model...\n",
            "Epoch 1: Train Loss = 6.7762\n",
            "Epoch 2: Train Loss = 6.1226\n",
            "Epoch 3: Train Loss = 5.8230\n",
            "Epoch 4: Train Loss = 5.5709\n",
            "Epoch 5: Train Loss = 5.3412\n",
            "Model configuration saved to model_config.txt\n",
            "Perplexity results saved to train_perplexity.txt\n",
            "Perplexity results saved to test_perplexity.txt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import math\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "###############################################\n",
        "# CONFIGURATION\n",
        "###############################################\n",
        "N_GRAM_SIZE = 3  # Explicitly provided n-gram size\n",
        "TRAIN_FILE = \"trainSetUlysses.txt\"\n",
        "TEST_FILE = \"testSetUlysses.txt\"\n",
        "VOCAB_FILE = \"vocabUlysses.pkl\"\n",
        "MODEL_FILE = \"ffnn_5.pth\"\n",
        "CONFIG_FILE = \"model_config.txt\"\n",
        "TRAIN_PERPLEXITY_FILE = \"train_perplexity.txt\"\n",
        "TEST_PERPLEXITY_FILE = \"test_perplexity.txt\"\n",
        "\n",
        "# Hyperparameters for model training\n",
        "EMBEDDING_DIM = 16  # Size of word embeddings\n",
        "HIDDEN_DIM = 64  # Number of neurons in the hidden layer\n",
        "BATCH_SIZE = 64 # Batch size for training\n",
        "LEARNING_RATE = 0.001  # Learning rate for the optimizer\n",
        "NUM_EPOCHS = 5  # Number of epochs for training\n",
        "PATIENCE = 3  # Early stopping patience\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "###############################################\n",
        "# LOAD VOCABULARY\n",
        "###############################################\n",
        "def loadVocabulary(vocabFile):\n",
        "    with open(vocabFile, \"rb\") as file:\n",
        "        wordToIndex, indexToWord = pickle.load(file)\n",
        "    return wordToIndex, indexToWord\n",
        "\n",
        "wordToIndex, indexToWord = loadVocabulary(VOCAB_FILE)\n",
        "vocabSize = len(wordToIndex)\n",
        "\n",
        "###############################################\n",
        "# DATASET PREPARATION\n",
        "###############################################\n",
        "class NGramDataset(Dataset):\n",
        "    def __init__(self, filePath, ngramSize, wordToIndex):\n",
        "        self.ngramSize = ngramSize\n",
        "        self.wordToIndex = wordToIndex\n",
        "        self.data = self.loadData(filePath)\n",
        "\n",
        "    def loadData(self, filePath):\n",
        "        data = []\n",
        "        with open(filePath, \"r\", encoding=\"utf-8\") as file:\n",
        "            for line in file:\n",
        "                words = line.strip().split()\n",
        "                if len(words) < self.ngramSize:\n",
        "                    words = [\"<PAD>\"] * (self.ngramSize - len(words)) + words\n",
        "                indices = [self.wordToIndex.get(w, self.wordToIndex[\"<UNK>\"]) for w in words]\n",
        "                for i in range(len(indices) - self.ngramSize + 1):\n",
        "                    context = indices[i:i + self.ngramSize - 1]\n",
        "                    target = indices[i + self.ngramSize - 1]\n",
        "                    data.append((context, target))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, target = self.data[idx]\n",
        "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "###############################################\n",
        "# DEFINE FFNN MODEL\n",
        "###############################################\n",
        "class FeedForwardLM(nn.Module):\n",
        "    def __init__(self, vocabSize, embedDim, contextSize, hiddenDim):\n",
        "        super(FeedForwardLM, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocabSize, embedDim)\n",
        "        self.linear1 = nn.Linear(contextSize * embedDim, hiddenDim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hiddenDim, vocabSize)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embeddings(inputs)\n",
        "        flat = embedded.view(embedded.size(0), -1)\n",
        "        hidden = self.activation(self.linear1(flat))\n",
        "        logits = self.linear2(hidden)\n",
        "        return logits\n",
        "\n",
        "###############################################\n",
        "# TRAINING FUNCTION\n",
        "###############################################\n",
        "def trainModel(model, dataLoader, optimizer, lossFunction, numEpochs, patience):\n",
        "    bestLoss = float(\"inf\")\n",
        "    noImprove = 0\n",
        "\n",
        "    for epoch in range(numEpochs):\n",
        "        model.train()\n",
        "        totalLoss = 0\n",
        "        for context, target in dataLoader:\n",
        "            context, target = context.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(context)\n",
        "            loss = lossFunction(logits, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            totalLoss += loss.item()\n",
        "\n",
        "        avgLoss = totalLoss / len(dataLoader)\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {avgLoss:.4f}\")\n",
        "\n",
        "        if avgLoss < bestLoss:\n",
        "            bestLoss = avgLoss\n",
        "            noImprove = 0\n",
        "        else:\n",
        "            noImprove += 1\n",
        "            if noImprove >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "                break\n",
        "\n",
        "###############################################\n",
        "# PERPLEXITY COMPUTATION\n",
        "###############################################\n",
        "def computePerplexity(filePath, model, wordToIndex, ngramSize, outputFile):\n",
        "    with open(filePath, \"r\", encoding=\"utf-8\") as file:\n",
        "        sentences = file.readlines()\n",
        "\n",
        "    totalLogProb = 0.0\n",
        "    totalCount = 0\n",
        "    model.eval()\n",
        "\n",
        "    with open(outputFile, \"w\", encoding=\"utf-8\") as fileOut:\n",
        "        for lineIndex, sentence in enumerate(sentences, start=1):\n",
        "            words = sentence.strip().split()\n",
        "            if len(words) < ngramSize:\n",
        "                words = [\"<PAD>\"] * (ngramSize - len(words)) + words\n",
        "            indices = [wordToIndex.get(w, wordToIndex[\"<UNK>\"]) for w in words]\n",
        "            logProb = 0.0\n",
        "            count = 0\n",
        "\n",
        "            for i in range(len(indices) - ngramSize + 1):\n",
        "                context = indices[i:i + ngramSize - 1]\n",
        "                target = indices[i + ngramSize - 1]\n",
        "                contextTensor = torch.tensor([context], dtype=torch.long).to(device)\n",
        "                with torch.no_grad():\n",
        "                    logits = model(contextTensor)\n",
        "                    logProbs = torch.nn.functional.log_softmax(logits, dim=1)\n",
        "                    logProb += logProbs[0, target].item()\n",
        "                count += 1\n",
        "\n",
        "            sentencePerplexity = math.exp(-logProb / count) if count > 0 else float(\"inf\")\n",
        "            fileOut.write(f\"Sentence {lineIndex}: Perplexity = {sentencePerplexity:.4f}\\n\")\n",
        "            totalLogProb += logProb\n",
        "            totalCount += count\n",
        "\n",
        "        overallPerplexity = math.exp(-totalLogProb / totalCount) if totalCount > 0 else float(\"inf\")\n",
        "        fileOut.write(f\"\\nOverall Perplexity: {overallPerplexity:.4f}\\n\")\n",
        "    print(f\"Perplexity results saved to {outputFile}\")\n",
        "\n",
        "###############################################\n",
        "# SAVE MODEL CONFIGURATION\n",
        "###############################################\n",
        "def saveModelConfig(configFile, vocabSize, embeddingDim, hiddenDim, batchSize, learningRate, numEpochs, patience, nGramSize):\n",
        "    with open(configFile, \"w\") as file:\n",
        "        file.write(f\"Vocab Size: {vocabSize}\\n\")\n",
        "        file.write(f\"Embedding Dim: {embeddingDim}\\n\")\n",
        "        file.write(f\"Hidden Dim: {hiddenDim}\\n\")\n",
        "        file.write(f\"Batch Size: {batchSize}\\n\")\n",
        "        file.write(f\"Learning Rate: {learningRate}\\n\")\n",
        "        file.write(f\"Num Epochs: {numEpochs}\\n\")\n",
        "        file.write(f\"Patience: {patience}\\n\")\n",
        "        file.write(f\"N-Gram Size: {nGramSize}\\n\")\n",
        "\n",
        "###############################################\n",
        "# EXECUTION\n",
        "###############################################\n",
        "def main():\n",
        "    # Load training dataset\n",
        "    trainDataset = NGramDataset(TRAIN_FILE, N_GRAM_SIZE, wordToIndex)\n",
        "    trainLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Initialize FFNN model\n",
        "    model = FeedForwardLM(vocabSize, EMBEDDING_DIM, N_GRAM_SIZE - 1, HIDDEN_DIM).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    lossFunction = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training FFNN model...\")\n",
        "    trainModel(model, trainLoader, optimizer, lossFunction, numEpochs=NUM_EPOCHS, patience=PATIENCE)\n",
        "    torch.save(model.state_dict(), MODEL_FILE)\n",
        "\n",
        "    # Save model configuration for reproducibility\n",
        "    saveModelConfig(CONFIG_FILE, vocabSize, EMBEDDING_DIM, HIDDEN_DIM, BATCH_SIZE, LEARNING_RATE, NUM_EPOCHS, PATIENCE, N_GRAM_SIZE)\n",
        "    print(f\"Model configuration saved to {CONFIG_FILE}\")\n",
        "\n",
        "    # Compute perplexity for training and testing data\n",
        "    computePerplexity(TRAIN_FILE, model, wordToIndex, N_GRAM_SIZE, TRAIN_PERPLEXITY_FILE)\n",
        "    computePerplexity(TEST_FILE, model, wordToIndex, N_GRAM_SIZE, TEST_PERPLEXITY_FILE)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs_QBa1JJPfC"
      },
      "source": [
        "RNN Vanilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI-X5h9hJTBW",
        "outputId": "0ad848ec-d3d5-4d3c-f716-a85a34fc5da5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RNN model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-c4333cfa3f98>:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  lengths = torch.tensor(lengths, dtype=torch.int64).cpu()  # for packing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss = 5.8639\n",
            "Epoch 2: Train Loss = 5.1047\n",
            "Epoch 3: Train Loss = 4.8219\n",
            "Epoch 4: Train Loss = 4.6127\n",
            "Epoch 5: Train Loss = 4.4375\n",
            "Epoch 6: Train Loss = 4.2848\n",
            "Epoch 7: Train Loss = 4.1458\n",
            "Model state_dict saved to rnn.pth\n",
            "Model configuration saved to model_config_rnn.txt\n",
            "Perplexity results saved to train_perplexity_rnn_Pride.txt\n",
            "Perplexity results saved to test_perplexity_rnn_Pride.txt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "###############################################\n",
        "# CONFIGURATION\n",
        "###############################################\n",
        "# Maximum total tokens in a segment.\n",
        "# Note: The effective input length is MAX_SEQ_LEN-1 because targets are shifted.\n",
        "MAX_SEQ_LEN = 50\n",
        "OVERLAP_RATIO = 0.5\n",
        "\n",
        "TRAIN_FILE = \"trainSetPride.txt\"\n",
        "TEST_FILE = \"testSetPride.txt\"\n",
        "VOCAB_FILE = \"vocabPride.pkl\"\n",
        "MODEL_FILE = \"rnn.pth\"\n",
        "CONFIG_FILE = \"model_config_rnn.txt\"\n",
        "TRAIN_PERPLEXITY_FILE = \"train_perplexity_rnn_Pride.txt\"\n",
        "TEST_PERPLEXITY_FILE = \"test_perplexity_rnn_Pride.txt\"\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 7\n",
        "PATIENCE = 3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "###############################################\n",
        "# LOAD VOCABULARY\n",
        "###############################################\n",
        "def loadVocabulary(vocabFile):\n",
        "    with open(vocabFile, \"rb\") as file:\n",
        "        wordToIndex, indexToWord = pickle.load(file)\n",
        "    for token in [\"<UNK>\", \"<PAD>\"]:\n",
        "        if token not in wordToIndex:\n",
        "            wordToIndex[token] = len(wordToIndex)\n",
        "            indexToWord[len(indexToWord)] = token\n",
        "    return wordToIndex, indexToWord\n",
        "\n",
        "wordToIndex, indexToWord = loadVocabulary(VOCAB_FILE)\n",
        "vocabSize = len(wordToIndex)\n",
        "\n",
        "###############################################\n",
        "# DATASET FOR NEXT-TOKEN PREDICTION\n",
        "###############################################\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, filePath, maxSeqLen, overlapRatio, wordToIndex):\n",
        "        \"\"\"\n",
        "        Each sample is a segment from the file.\n",
        "        For a segment of tokens [w1, w2, ..., w_T] (with T <= maxSeqLen),\n",
        "        we create:\n",
        "          - input:  [w1, w2, ..., w_{T-1}]\n",
        "          - target: [w2, w3, ..., w_T]\n",
        "        Both sequences are padded to a fixed length of (maxSeqLen - 1) if needed.\n",
        "        \"\"\"\n",
        "        self.maxSeqLen = maxSeqLen  # total tokens in segment\n",
        "        self.overlapSize = int(maxSeqLen * overlapRatio)\n",
        "        self.wordToIndex = wordToIndex\n",
        "        self.data, self.lengths = self.loadData(filePath)\n",
        "\n",
        "    def loadData(self, filePath):\n",
        "        data = []      # list of tuples: (input_seq, target_seq)\n",
        "        lengths = []   # effective lengths (without padding) for input sequences\n",
        "        with open(filePath, \"r\", encoding=\"utf-8\") as file:\n",
        "            for line in file:\n",
        "                words = line.strip().split()\n",
        "                if len(words) < 2:\n",
        "                    continue  # need at least 2 tokens for input and target\n",
        "                wordIndices = [self.wordToIndex.get(w, self.wordToIndex[\"<UNK>\"]) for w in words]\n",
        "                # If the sentence is short enough, use it whole\n",
        "                if len(wordIndices) <= self.maxSeqLen:\n",
        "                    input_seq = wordIndices[:-1]\n",
        "                    target_seq = wordIndices[1:]\n",
        "                    effective_length = len(input_seq)  # equals len(wordIndices)-1\n",
        "                    if effective_length < self.maxSeqLen - 1:\n",
        "                        pad_length = (self.maxSeqLen - 1) - effective_length\n",
        "                        input_seq = input_seq + [self.wordToIndex[\"<PAD>\"]] * pad_length\n",
        "                        target_seq = target_seq + [self.wordToIndex[\"<PAD>\"]] * pad_length\n",
        "                    data.append((input_seq, target_seq))\n",
        "                    lengths.append(effective_length)\n",
        "                else:\n",
        "                    # For long sentences, break into overlapping segments.\n",
        "                    for i in range(0, len(wordIndices), self.overlapSize):\n",
        "                        segment = wordIndices[i:i+self.maxSeqLen]\n",
        "                        if len(segment) < 2:\n",
        "                            continue\n",
        "                        input_seq = segment[:-1]\n",
        "                        target_seq = segment[1:]\n",
        "                        effective_length = len(input_seq)\n",
        "                        if effective_length < self.maxSeqLen - 1:\n",
        "                            pad_length = (self.maxSeqLen - 1) - effective_length\n",
        "                            input_seq = input_seq + [self.wordToIndex[\"<PAD>\"]] * pad_length\n",
        "                            target_seq = target_seq + [self.wordToIndex[\"<PAD>\"]] * pad_length\n",
        "                        data.append((input_seq, target_seq))\n",
        "                        lengths.append(effective_length)\n",
        "        return data, lengths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq, target_seq = self.data[idx]\n",
        "        length = self.lengths[idx]\n",
        "        return (torch.tensor(input_seq, dtype=torch.long),\n",
        "                torch.tensor(target_seq, dtype=torch.long),\n",
        "                length)\n",
        "\n",
        "###############################################\n",
        "# RNN LANGUAGE MODEL\n",
        "###############################################\n",
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, vocabSize, embedDim, hiddenDim):\n",
        "        super(RNNLanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocabSize, embedDim, padding_idx=wordToIndex[\"<PAD>\"])\n",
        "        self.rnn = nn.RNN(embedDim, hiddenDim, batch_first=True)\n",
        "        self.fc = nn.Linear(hiddenDim, vocabSize)\n",
        "\n",
        "    def forward(self, inputs, lengths):\n",
        "        embedded = self.embedding(inputs)\n",
        "        # Pack the embedded sequence (lengths must be on CPU)\n",
        "        packedInputs = rnn_utils.pack_padded_sequence(embedded, lengths.cpu(),\n",
        "                                                      batch_first=True, enforce_sorted=False)\n",
        "        packedOutputs, _ = self.rnn(packedInputs)\n",
        "        outputs, _ = rnn_utils.pad_packed_sequence(packedOutputs, batch_first=True,\n",
        "                                                   total_length=inputs.size(1))\n",
        "        logits = self.fc(outputs)\n",
        "        return logits\n",
        "\n",
        "###############################################\n",
        "# TRAINING FUNCTION\n",
        "###############################################\n",
        "def trainModel(model, trainLoader, optimizer, lossFunction, numEpochs, patience):\n",
        "    bestLoss = float(\"inf\")\n",
        "    noImprove = 0\n",
        "    for epoch in range(numEpochs):\n",
        "        model.train()\n",
        "        totalLoss = 0.0\n",
        "        for inputs, targets, lengths in trainLoader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            lengths = torch.tensor(lengths, dtype=torch.int64).cpu()  # for packing\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(inputs, lengths)  # shape: [batch, seq_len, vocabSize]\n",
        "            logits_flat = logits.view(-1, vocabSize)\n",
        "            targets_flat = targets.view(-1)\n",
        "            loss = lossFunction(logits_flat, targets_flat)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            totalLoss += loss.item()\n",
        "\n",
        "        avgLoss = totalLoss / len(trainLoader)\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {avgLoss:.4f}\")\n",
        "\n",
        "        if avgLoss < bestLoss:\n",
        "            bestLoss = avgLoss\n",
        "            noImprove = 0\n",
        "        else:\n",
        "            noImprove += 1\n",
        "            if noImprove >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "###############################################\n",
        "# PERPLEXITY COMPUTATION\n",
        "###############################################\n",
        "def computePerplexity(filePath, model, wordToIndex, maxSeqLen, outFile):\n",
        "    \"\"\"\n",
        "    For each sentence in the file, create an input (all but the last token) and\n",
        "    target (all but the first token), pad if needed, and compute the perplexity\n",
        "    only on the non-padded (effective) tokens.\n",
        "    \"\"\"\n",
        "    with open(filePath, \"r\", encoding=\"utf-8\") as file:\n",
        "        sentences = [line.strip().split() for line in file if line.strip()]\n",
        "\n",
        "    totalLogProb = 0.0\n",
        "    totalCount = 0\n",
        "\n",
        "    model.eval()\n",
        "    with open(outFile, \"w\", encoding=\"utf-8\") as fout:\n",
        "        for i, words in enumerate(sentences):\n",
        "            if len(words) < 2:\n",
        "                continue  # skip sentences that are too short\n",
        "            words = words[:maxSeqLen]\n",
        "            indices = [wordToIndex.get(w, wordToIndex[\"<UNK>\"]) for w in words]\n",
        "            effective_length = len(indices) - 1  # number of predictions\n",
        "            input_seq = indices[:-1]\n",
        "            target_seq = indices[1:]\n",
        "            if effective_length < maxSeqLen - 1:\n",
        "                pad_length = (maxSeqLen - 1) - effective_length\n",
        "                input_seq = input_seq + [wordToIndex[\"<PAD>\"]] * pad_length\n",
        "                target_seq = target_seq + [wordToIndex[\"<PAD>\"]] * pad_length\n",
        "\n",
        "            inputs = torch.tensor([input_seq], dtype=torch.long).to(device)\n",
        "            lengths = torch.tensor([effective_length], dtype=torch.int64).cpu()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(inputs, lengths)\n",
        "                logProbs = nn.functional.log_softmax(logits, dim=2)\n",
        "                logProbSum = 0.0\n",
        "                for j in range(effective_length):\n",
        "                    logProbSum += logProbs[0, j, target_seq[j]].item()\n",
        "\n",
        "            sentPerplexity = math.exp(-logProbSum / effective_length)\n",
        "            fout.write(f\"Sentence {i+1}: Perplexity = {sentPerplexity:.4f}\\n\")\n",
        "            totalLogProb += logProbSum\n",
        "            totalCount += effective_length\n",
        "\n",
        "        overallPerplexity = math.exp(-totalLogProb / totalCount) if totalCount > 0 else float(\"inf\")\n",
        "        fout.write(f\"\\nOverall Perplexity: {overallPerplexity:.4f}\\n\")\n",
        "\n",
        "    print(f\"Perplexity results saved to {outFile}\")\n",
        "\n",
        "###############################################\n",
        "# SAVE MODEL CONFIGURATION\n",
        "###############################################\n",
        "def saveModelInfo(configFile, vocabSize, embedDim, hiddenDim, maxSeqLen, overlapRatio,\n",
        "                  learningRate, numEpochs, patience):\n",
        "    with open(configFile, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"MODEL CONFIGURATION\\n\")\n",
        "        f.write(\"===================\\n\")\n",
        "        f.write(f\"Vocabulary Size: {vocabSize}\\n\")\n",
        "        f.write(f\"Embedding Dimension: {embedDim}\\n\")\n",
        "        f.write(f\"Hidden Dimension: {hiddenDim}\\n\")\n",
        "        f.write(f\"Max Sequence Length: {maxSeqLen}\\n\")\n",
        "        f.write(f\"Overlap Ratio: {overlapRatio}\\n\")\n",
        "        f.write(f\"Learning Rate: {learningRate}\\n\")\n",
        "        f.write(f\"Number of Epochs: {numEpochs}\\n\")\n",
        "        f.write(f\"Patience: {patience}\\n\")\n",
        "    print(f\"Model configuration saved to {configFile}\")\n",
        "\n",
        "###############################################\n",
        "# MAIN EXECUTION\n",
        "###############################################\n",
        "def main():\n",
        "    # Prepare dataset and loader.\n",
        "    trainDataset = SentenceDataset(TRAIN_FILE, MAX_SEQ_LEN, OVERLAP_RATIO, wordToIndex)\n",
        "    trainLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    model = RNNLanguageModel(vocabSize, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    lossFunction = nn.CrossEntropyLoss(ignore_index=wordToIndex[\"<PAD>\"])\n",
        "\n",
        "    print(\"Training RNN model...\")\n",
        "    trainModel(model, trainLoader, optimizer, lossFunction, NUM_EPOCHS, PATIENCE)\n",
        "\n",
        "    # Save the trained model.\n",
        "    torch.save(model.state_dict(), MODEL_FILE)\n",
        "    print(f\"Model state_dict saved to {MODEL_FILE}\")\n",
        "\n",
        "    # Save model configuration/info.\n",
        "    saveModelInfo(CONFIG_FILE, vocabSize, EMBEDDING_DIM, HIDDEN_DIM, MAX_SEQ_LEN,\n",
        "                  OVERLAP_RATIO, LEARNING_RATE, NUM_EPOCHS, PATIENCE)\n",
        "\n",
        "    # Compute perplexity on train and test sets.\n",
        "    computePerplexity(TRAIN_FILE, model, wordToIndex, MAX_SEQ_LEN, TRAIN_PERPLEXITY_FILE)\n",
        "    computePerplexity(TEST_FILE, model, wordToIndex, MAX_SEQ_LEN, TEST_PERPLEXITY_FILE)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKmtiIRGa8nX"
      },
      "source": [
        "**LSTMCode**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EtQGtHLbBt3",
        "outputId": "687488bd-412e-4967-b896-948efac36a53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSTM model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-3ddf69f4f8bf>:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  lengths = torch.tensor(lengths, dtype=torch.int64).cpu()  # for packing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss = 6.6250\n",
            "Epoch 2: Train Loss = 5.8983\n",
            "Epoch 3: Train Loss = 5.5295\n",
            "Epoch 4: Train Loss = 5.2154\n",
            "Epoch 5: Train Loss = 4.9427\n",
            "Epoch 6: Train Loss = 4.7039\n",
            "Epoch 7: Train Loss = 4.4811\n",
            "Model state_dict saved to lstm.pth\n",
            "Model configuration saved to model_config_lstm.txt\n",
            "Perplexity results saved to train_perplexity_lstm_Ulysses.txt\n",
            "Perplexity results saved to test_perplexity_lstm_Ulysses.txt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "###############################################\n",
        "# CONFIGURATION\n",
        "###############################################\n",
        "# Maximum total tokens in a segment.\n",
        "# Note: The effective input length is MAX_SEQ_LEN-1 because targets are shifted.\n",
        "MAX_SEQ_LEN = 50\n",
        "OVERLAP_RATIO = 0.5\n",
        "\n",
        "TRAIN_FILE = \"trainSetUlysses.txt\"\n",
        "TEST_FILE = \"testSetUlysses.txt\"\n",
        "VOCAB_FILE = \"vocabUlysses.pkl\"\n",
        "MODEL_FILE = \"lstm.pth\"\n",
        "CONFIG_FILE = \"model_config_lstm.txt\"\n",
        "TRAIN_PERPLEXITY_FILE = \"train_perplexity_lstm_Ulysses.txt\"\n",
        "TEST_PERPLEXITY_FILE = \"test_perplexity_lstm_Ulysses.txt\"\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 7\n",
        "PATIENCE = 3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "###############################################\n",
        "# LOAD VOCABULARY\n",
        "###############################################\n",
        "def loadVocabulary(vocabFile):\n",
        "    with open(vocabFile, \"rb\") as file:\n",
        "        wordToIndex, indexToWord = pickle.load(file)\n",
        "    for token in [\"<UNK>\", \"<PAD>\"]:\n",
        "        if token not in wordToIndex:\n",
        "            wordToIndex[token] = len(wordToIndex)\n",
        "            indexToWord[len(indexToWord)] = token\n",
        "    return wordToIndex, indexToWord\n",
        "\n",
        "wordToIndex, indexToWord = loadVocabulary(VOCAB_FILE)\n",
        "vocabSize = len(wordToIndex)\n",
        "\n",
        "###############################################\n",
        "# DATASET FOR NEXT-TOKEN PREDICTION\n",
        "###############################################\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, filePath, maxSeqLen, overlapRatio, wordToIndex):\n",
        "        \"\"\"\n",
        "        Each sample is a segment from the file.\n",
        "        For a segment of tokens [w1, w2, ..., w_T] (with T <= maxSeqLen),\n",
        "        we create:\n",
        "          - input:  [w1, w2, ..., w_{T-1}]\n",
        "          - target: [w2, w3, ..., w_T]\n",
        "        Both sequences are padded to a fixed length of (maxSeqLen - 1) if needed.\n",
        "        \"\"\"\n",
        "        self.maxSeqLen = maxSeqLen  # total tokens in segment\n",
        "        self.overlapSize = int(maxSeqLen * overlapRatio)\n",
        "        self.wordToIndex = wordToIndex\n",
        "        self.data, self.lengths = self.loadData(filePath)\n",
        "\n",
        "    def loadData(self, filePath):\n",
        "        data = []      # list of tuples: (input_seq, target_seq)\n",
        "        lengths = []   # effective lengths (without padding) for input sequences\n",
        "        with open(filePath, \"r\", encoding=\"utf-8\") as file:\n",
        "            for line in file:\n",
        "                words = line.strip().split()\n",
        "                if len(words) < 2:\n",
        "                    continue  # need at least 2 tokens for input and target\n",
        "                wordIndices = [self.wordToIndex.get(w, self.wordToIndex[\"<UNK>\"]) for w in words]\n",
        "                # If the sentence is short enough, use it whole\n",
        "                if len(wordIndices) <= self.maxSeqLen:\n",
        "                    input_seq = wordIndices[:-1]\n",
        "                    target_seq = wordIndices[1:]\n",
        "                    effective_length = len(input_seq)  # equals len(wordIndices)-1\n",
        "                    if effective_length < self.maxSeqLen - 1:\n",
        "                        pad_length = (self.maxSeqLen - 1) - effective_length\n",
        "                        input_seq = input_seq + [self.wordToIndex[\"<PAD>\"]] * pad_length\n",
        "                        target_seq = target_seq + [self.wordToIndex[\"<PAD>\"]] * pad_length\n",
        "                    data.append((input_seq, target_seq))\n",
        "                    lengths.append(effective_length)\n",
        "                else:\n",
        "                    # For long sentences, break into overlapping segments.\n",
        "                    for i in range(0, len(wordIndices), self.overlapSize):\n",
        "                        segment = wordIndices[i:i+self.maxSeqLen]\n",
        "                        if len(segment) < 2:\n",
        "                            continue\n",
        "                        input_seq = segment[:-1]\n",
        "                        target_seq = segment[1:]\n",
        "                        effective_length = len(input_seq)\n",
        "                        if effective_length < self.maxSeqLen - 1:\n",
        "                            pad_length = (self.maxSeqLen - 1) - effective_length\n",
        "                            input_seq = input_seq + [self.wordToIndex[\"<PAD>\"]] * pad_length\n",
        "                            target_seq = target_seq + [self.wordToIndex[\"<PAD>\"]] * pad_length\n",
        "                        data.append((input_seq, target_seq))\n",
        "                        lengths.append(effective_length)\n",
        "        return data, lengths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq, target_seq = self.data[idx]\n",
        "        length = self.lengths[idx]\n",
        "        return (torch.tensor(input_seq, dtype=torch.long),\n",
        "                torch.tensor(target_seq, dtype=torch.long),\n",
        "                length)\n",
        "\n",
        "###############################################\n",
        "# LSTM LANGUAGE MODEL\n",
        "###############################################\n",
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocabSize, embedDim, hiddenDim):\n",
        "        super(LSTMLanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocabSize, embedDim, padding_idx=wordToIndex[\"<PAD>\"])\n",
        "        self.lstm = nn.LSTM(embedDim, hiddenDim, batch_first=True)\n",
        "        self.fc = nn.Linear(hiddenDim, vocabSize)\n",
        "\n",
        "    def forward(self, inputs, lengths):\n",
        "        embedded = self.embedding(inputs)\n",
        "        # Pack the embedded sequence (lengths must be on CPU)\n",
        "        packedInputs = rnn_utils.pack_padded_sequence(embedded, lengths.cpu(),\n",
        "                                                      batch_first=True, enforce_sorted=False)\n",
        "        packedOutputs, (h_n, c_n) = self.lstm(packedInputs)\n",
        "        outputs, _ = rnn_utils.pad_packed_sequence(packedOutputs, batch_first=True,\n",
        "                                                   total_length=inputs.size(1))\n",
        "        logits = self.fc(outputs)\n",
        "        return logits\n",
        "\n",
        "###############################################\n",
        "# TRAINING FUNCTION\n",
        "###############################################\n",
        "def trainModel(model, trainLoader, optimizer, lossFunction, numEpochs, patience):\n",
        "    bestLoss = float(\"inf\")\n",
        "    noImprove = 0\n",
        "    for epoch in range(numEpochs):\n",
        "        model.train()\n",
        "        totalLoss = 0.0\n",
        "        for inputs, targets, lengths in trainLoader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            lengths = torch.tensor(lengths, dtype=torch.int64).cpu()  # for packing\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(inputs, lengths)  # shape: [batch, seq_len, vocabSize]\n",
        "            logits_flat = logits.view(-1, vocabSize)\n",
        "            targets_flat = targets.view(-1)\n",
        "            loss = lossFunction(logits_flat, targets_flat)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            totalLoss += loss.item()\n",
        "\n",
        "        avgLoss = totalLoss / len(trainLoader)\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {avgLoss:.4f}\")\n",
        "\n",
        "        if avgLoss < bestLoss:\n",
        "            bestLoss = avgLoss\n",
        "            noImprove = 0\n",
        "        else:\n",
        "            noImprove += 1\n",
        "            if noImprove >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "###############################################\n",
        "# PERPLEXITY COMPUTATION\n",
        "###############################################\n",
        "def computePerplexity(filePath, model, wordToIndex, maxSeqLen, outFile):\n",
        "    \"\"\"\n",
        "    For each sentence in the file, create an input (all but the last token) and\n",
        "    target (all but the first token), pad if needed, and compute the perplexity\n",
        "    only on the non-padded (effective) tokens.\n",
        "    \"\"\"\n",
        "    with open(filePath, \"r\", encoding=\"utf-8\") as file:\n",
        "        sentences = [line.strip().split() for line in file if line.strip()]\n",
        "\n",
        "    totalLogProb = 0.0\n",
        "    totalCount = 0\n",
        "\n",
        "    model.eval()\n",
        "    with open(outFile, \"w\", encoding=\"utf-8\") as fout:\n",
        "        for i, words in enumerate(sentences):\n",
        "            if len(words) < 2:\n",
        "                continue  # skip sentences that are too short\n",
        "            words = words[:maxSeqLen]\n",
        "            indices = [wordToIndex.get(w, wordToIndex[\"<UNK>\"]) for w in words]\n",
        "            effective_length = len(indices) - 1  # number of predictions\n",
        "            input_seq = indices[:-1]\n",
        "            target_seq = indices[1:]\n",
        "            if effective_length < maxSeqLen - 1:\n",
        "                pad_length = (maxSeqLen - 1) - effective_length\n",
        "                input_seq = input_seq + [wordToIndex[\"<PAD>\"]] * pad_length\n",
        "                target_seq = target_seq + [wordToIndex[\"<PAD>\"]] * pad_length\n",
        "\n",
        "            inputs = torch.tensor([input_seq], dtype=torch.long).to(device)\n",
        "            lengths = torch.tensor([effective_length], dtype=torch.int64).cpu()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(inputs, lengths)\n",
        "                logProbs = nn.functional.log_softmax(logits, dim=2)\n",
        "                logProbSum = 0.0\n",
        "                for j in range(effective_length):\n",
        "                    logProbSum += logProbs[0, j, target_seq[j]].item()\n",
        "\n",
        "            sentPerplexity = math.exp(-logProbSum / effective_length)\n",
        "            fout.write(f\"Sentence {i+1}: Perplexity = {sentPerplexity:.4f}\\n\")\n",
        "            totalLogProb += logProbSum\n",
        "            totalCount += effective_length\n",
        "\n",
        "        overallPerplexity = math.exp(-totalLogProb / totalCount) if totalCount > 0 else float(\"inf\")\n",
        "        fout.write(f\"\\nOverall Perplexity: {overallPerplexity:.4f}\\n\")\n",
        "\n",
        "    print(f\"Perplexity results saved to {outFile}\")\n",
        "\n",
        "###############################################\n",
        "# SAVE MODEL CONFIGURATION\n",
        "###############################################\n",
        "def saveModelInfo(configFile, vocabSize, embedDim, hiddenDim, maxSeqLen, overlapRatio,\n",
        "                  learningRate, numEpochs, patience):\n",
        "    with open(configFile, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"MODEL CONFIGURATION\\n\")\n",
        "        f.write(\"===================\\n\")\n",
        "        f.write(f\"Vocabulary Size: {vocabSize}\\n\")\n",
        "        f.write(f\"Embedding Dimension: {embedDim}\\n\")\n",
        "        f.write(f\"Hidden Dimension: {hiddenDim}\\n\")\n",
        "        f.write(f\"Max Sequence Length: {maxSeqLen}\\n\")\n",
        "        f.write(f\"Overlap Ratio: {overlapRatio}\\n\")\n",
        "        f.write(f\"Learning Rate: {learningRate}\\n\")\n",
        "        f.write(f\"Number of Epochs: {numEpochs}\\n\")\n",
        "        f.write(f\"Patience: {patience}\\n\")\n",
        "    print(f\"Model configuration saved to {configFile}\")\n",
        "\n",
        "###############################################\n",
        "# MAIN EXECUTION\n",
        "###############################################\n",
        "def main():\n",
        "    # Prepare dataset and loader.\n",
        "    trainDataset = SentenceDataset(TRAIN_FILE, MAX_SEQ_LEN, OVERLAP_RATIO, wordToIndex)\n",
        "    trainLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    model = LSTMLanguageModel(vocabSize, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    lossFunction = nn.CrossEntropyLoss(ignore_index=wordToIndex[\"<PAD>\"])\n",
        "\n",
        "    print(\"Training LSTM model...\")\n",
        "    trainModel(model, trainLoader, optimizer, lossFunction, NUM_EPOCHS, PATIENCE)\n",
        "\n",
        "    # Save the trained model.\n",
        "    torch.save(model.state_dict(), MODEL_FILE)\n",
        "    print(f\"Model state_dict saved to {MODEL_FILE}\")\n",
        "\n",
        "    # Save model configuration/info.\n",
        "    saveModelInfo(CONFIG_FILE, vocabSize, EMBEDDING_DIM, HIDDEN_DIM, MAX_SEQ_LEN,\n",
        "                  OVERLAP_RATIO, LEARNING_RATE, NUM_EPOCHS, PATIENCE)\n",
        "\n",
        "    # Compute perplexity on train and test sets.\n",
        "    computePerplexity(TRAIN_FILE, model, wordToIndex, MAX_SEQ_LEN, TRAIN_PERPLEXITY_FILE)\n",
        "    computePerplexity(TEST_FILE, model, wordToIndex, MAX_SEQ_LEN, TEST_PERPLEXITY_FILE)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
